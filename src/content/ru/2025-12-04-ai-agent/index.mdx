---
title: 'Как мы внедряли ИИ в FinOps дашборд'
description: 'Как ИИ-агент в FinOps-дашборде теперь помогает нам анализировать траты.'
pubDate: 'Dec 4 2025'
tags: ['AI', 'MCP']
cover: 'cover.webp'
lang: 'ru'
---

И до нас дошла ИИ истерия – мы внедрили агента в FinOps дашборд, о котором я рассказывал в течение этого года.

Несмотря на неоднозначное отношение к ИИ в обществе, конкретно это применение ИИ я считаю действительно полезным. Но обо всём по порядку.

## Зачем агент в дашборде

Агент способен делать аналитику быстрее, чем человек. Представьте, вместо того чтобы аналитику вручную крутить таблицы, анализируя траты, любой пользователь приложения может спросить обычным человеческим языком ИИ-агента и получить ответ на таком же человеческом языке.

К примеру, менеджер может задать вопрос: “Почему у нас был резкий скачок трат в октябре 2025 года?”. Сколько времени будет потрачено, если поручить эту задачу человеку? Несколько десятков минут? Или несколько часов? ИИ-агент может подготовить аналитику меньше чем за минуту. Но чтобы агент мог это делать, ему нужно уметь ходить в наши данные и для этого есть MCP-серверы.

## Как работает агент и что такое MCP

Главная часть, связывающая приложение с агентом, – это MCP ([Model Context Protocol](https://modelcontextprotocol.io/docs/getting-started/intro)), специальный интерфейс, с которым может работать агент. 

Проще всего думать об MCP как о REST API для ИИ. Вместо привычного UI-клиента у вас LLM, а вместо контроллеров — «инструменты» MCP-сервера. Агент видит список инструментов и сам решает, какой инструмент вызвать, подставляя аргументы и обрабатывая ответ.

В качестве инструментов может быть что угодно: функция, которая отправляет запросы к базе данных, любой API и так далее. Разработчики библиотек уже делают MCP-серверы вокруг своих продуктов, чтобы агенты в наших IDE могли получать самую свежую информацию об их использовании. Например, есть MCP-серверы [для Next.js](https://nextjs.org/docs/app/guides/mcp), [Chakra UI](https://chakra-ui.com/docs/get-started/ai/mcp-server), [React Suite](https://rsuitejs.com/guide/mcp-server/).

## Архитектура решения и жизненный цикл запроса

На диаграмме показано, как проходит типичный запрос.

1. Пользователь задаёт вопрос в интерфейсе дашборда.  
2. Бэкенд добавляет к нему системный промпт (кто такой агент, с какими данными он работает, какие у него ограничения) и отправляет диалог агенту с включённым MCP.  
3. Агент анализирует вопрос, выбирает нужный инструмент MCP, генерирует SQL-запрос и отправляет его на MCP-сервер.  
4. MCP-сервер выполняет SQL в базе от имени read-only пользователя и возвращает результат или ошибку.  
5. Агент интерпретирует данные, формирует человеческий ответ и бэкенд отдаёт его пользователю.

## Настройка агента и безопасность

1. Если планируете использовать сторонний MCP-сервер, то обязательно проверьте его на наличие уязвимостей. Например, MCP-сервер для Postgres от Anthropic какое-то время [имел уязвимость к SQL инъекциям](https://securitylabs.datadoghq.com/articles/mcp-vulnerability-case-study-SQL-injection-in-the-postgresql-mcp-server/).

2. Выдавайте для MCP-сервера доступ только на чтение и [только на те данные, которые нужны для выполнения задачи](https://ru.wikipedia.org/wiki/%D0%9F%D1%80%D0%B8%D0%BD%D1%86%D0%B8%D0%BF_%D0%BC%D0%B8%D0%BD%D0%B8%D0%BC%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D1%85_%D0%BF%D1%80%D0%B8%D0%B2%D0%B8%D0%BB%D0%B5%D0%B3%D0%B8%D0%B9). В идеале, это должно быть сделано на уровне привилегий пользователя, который будет использоваться в MCP-сервере.

3. Подбирайте reasoning (способность модели анализировать информацию), отвечающий вашим требованиям. Чем выше reasoning, тем осмысленнее будут ответы модели, но тем дольше будет обрабатываться запрос. Мы используем GPT-5 mini и дефолтный reasoning приводил к тому, что запросы обрабатывались около полуминуты. Для нас это было слишком долго, поэтому мы снизили до низкого уровня. Этого оказалось достаточно для генерации SQL-запросов.

4. Системный промпт может быть очень большим и это нормально. Лучше максимально подробно объяснять схему базы данных, типы данных, показывать примеры запросов, формат для результатов и так далее. Иначе существующие LLM ошибаются, извлекают не те данные или неправильно их интерпретируют. Например, наш системный промпт занимает около 24 тысячи символов (500+ строк).

5. Приготовьтесь к тому, что, скорее всего, вам придётся городить костыли. Все инструменты и библиотеки связанные с MCP сырые. Например, нам пришлось запускать MCP-сервер в одном контейнере с приложением, потому что MCP-сервер для Azure от Microsoft [поддерживает только](https://github.com/microsoft/mcp/issues/479) stdio.

# Заключение

С технической стороны реализация аналогичного приложения достаточно проста – это обычный враппер над GPT с MCP-сервером и read-only доступом к БД. Но даже такое приложение способно экономить человеко-часы – агент сильно сокращает путь от вопроса до первых цифр. Теперь ответы на такие вопросы можно получить уже через минуту. Главное не доверять слепо ИИ и перепроверять результаты.